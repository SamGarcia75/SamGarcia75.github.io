<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <body>
            <title>Partie 3 : Machine Learning et DCP </title>
        <link rel="stylesheet" type="text/css" href="CSS\style_div.css"> 
        <div id="bandeau"><h1>Partie 3 : Machine Learning et DCP</h1></div>
        <div id="contenu">
        <p>
            <br> <br>Le machine Learning lié au Big Data met fin au paradigme de 
            protection parrestriction de l’accès aux données. L’anonymisation 
            de la donnée peutdevenir totalement inutile face à la puissance d’un 
            algorithme de machinelearning. Concrètement, le chercheur démontre la 
            capacité pour unalgorithme d’établir des relations inférentielles entre données. 
            Ces relations permettent à l’utilisateur de l’algorithme de « remonter » à une personne à partir de la seule relation entre des données anonymisées.
            <br> <br>L’enjeu juridique posé par cette situation est le suivant : 
            les relations inférentielles peuvent-elles être considérées comme des donnéespersonnelles en tant que telles ?
            Il est important de prendre la mesure du problème, les outils de ML s’appuyant sur ces « relations/données » sont 
            exploités à des fins commerciales, de suivi comportemental voire la définition pure et simple dequi nous sommes. 
            Ces outils échappent à l’heure actuelle à tout contrôle, carsont exclus du champ d’application du RGPD à cause de la prétendue anonymisation des données exploitées.    
           <br> <br> 
           Car les modèles exploités par le machine learning sont nourris par desdonnées privées et parfois sensibles, afin d’optimiser des résultats toujours 
           plus pertinents, les risques liés à ces modèles sont en forte croissance. Si lesprogrammes de machine learning font un usage important de données 
           anonymisées, stockées de manière plus que sécurisée, il est particulièrementdur d’offrir les mêmes garanties sur les modèles exploités par le machine 
           learning lui-même. 
           <br>Concrètement, avant d’être traitées, les données sontdifficilement sécurisables, après traitement, elles sont anonymisées etsécurisées.
            L’état de l’art permet de développer des méthodes d’attaques sur cesmodèles. Une attaque de modèle de Machine Learning est un détournementde son usage, par le contournement 
            de leurs fonctionnalités ou par larévélation de ses secrets. Ce dernier type d’attaque, appelé « privacy attack »ou attaque contre la confidentialité. C’est celui qui nous intéresse le plus dansle cadre de la protection des données à caractère personnel.
            
            <br> <br>
           
        Les attaques par inversion de modèle sont les plus courantes, les plus fiables et les plusfaciles à réaliser en raison du volume de données offert par le Big Data. 
        Pour envisager cette attaque il faut comprendre à quel genre d’anonymisation procède le créateur de modèles de machine learning. Quand elle passe dans la black-box 
        (coeur du programmede l’IA, difficilement envisageable et intelligible pour l’esprit humain), la machineprocède à un hash ou à un chiffrement robuste de la donnée. 
        Cela signifie par exemplequ’une photo précise devient un amas de pixels absolument indéchiffrable pour l’oeilhumain. C’est la technique utilisée dans le cadre des NFT par exemple. 
        <br> <br> L’attaque intervient là, grâce au volume de données publiques (photos entre autres) du Big data,l’assaillant peut être en mesure de reconstituer une photo "hachée".
        Cette possibilité offerte à l’attaquant de retrouver une donnée à caractère personnel àpartir du seul outil lui-même impose que la destruction pure et simple de la donnée n’est 
        pas suffisante pour la protéger…
        <br> <br> Cette épineuse question des attaques de modèle pose un enjeu fondamental en matièrede protection de la vie privée et des données à caractère personnel.
        En effet, un des éléments caractéristiques des textes juridiques sur ce sujet est la duréede conservation des données personnelles. 
        Le simple fait qu’une attaque par inversionde modèles, par extraction ou sur les données d’entraînement, soit capable de «remonter » des données à caractère personnel 
        signifie par extension que la suppressionpure et simple d’une donnée pas inutilisable. 
        En outre, cette démonstration faite, ilconvient de s’interroger sur la pertinence de la définition actuelle de la « donnée personnelle ».
        </p>
        </div>    
            <div id="piedpage"><div><a href="conclusion.html">Partie suivante : Conclusion </a></div></div>
    </head>
</body>
</html>